# üìä B√ÅO C√ÅO D·ª∞ √ÅN: D·ª∞ ƒêO√ÅN S·ª®C M·∫†NH ƒê·ªíNG USD
## H∆∞·ªõng d·∫´n thi·∫øt k·∫ø Slide thuy·∫øt tr√¨nh

---

## üìå T·ªîNG QUAN D·ª∞ √ÅN

| Th√¥ng tin | Chi ti·∫øt |
|-----------|----------|
| **T√™n d·ª± √°n** | Exchange Rate Prediction - D·ª± ƒëo√°n t·ª∑ gi√° ngo·∫°i h·ªëi |
| **M·ª•c ti√™u** | D·ª± ƒëo√°n s·ª©c m·∫°nh ƒë·ªìng USD trong 7 ng√†y t∆∞∆°ng lai |
| **D·ªØ li·ªáu** | T·ª∑ gi√° c√°c ƒë·ªìng ti·ªÅn so v·ªõi USD t·ª´ 2004-2026 |
| **M√¥ h√¨nh** | ARIMA (Baseline) ‚Üí LSTM ‚Üí Transformer |

---

## üóÇÔ∏è C·∫§U TR√öC TH∆Ø M·ª§C D·ª∞ √ÅN

```
exchange-rate/
‚îú‚îÄ‚îÄ config/                     # C·∫•u h√¨nh
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml            # Tham s·ªë: horizon=7, window=30
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                   # D·ªØ li·ªáu g·ªëc
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ exchange_rate_to_usd.csv
‚îÇ   ‚îî‚îÄ‚îÄ processed/             # D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
‚îÇ       ‚îú‚îÄ‚îÄ merged/
‚îÇ       ‚îú‚îÄ‚îÄ usd_lstm_transformer/
‚îÇ       ‚îî‚îÄ‚îÄ model_ready_arima_sarima/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ craw_data/            # Thu th·∫≠p d·ªØ li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/         # Ti·ªÅn x·ª≠ l√Ω
‚îÇ   ‚îú‚îÄ‚îÄ eda/                   # Ph√¢n t√≠ch kh√°m ph√°
‚îÇ   ‚îú‚îÄ‚îÄ models/                # M√¥ h√¨nh ML
‚îÇ   ‚îú‚îÄ‚îÄ training/              # Hu·∫•n luy·ªán
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/            # ƒê√°nh gi√°
‚îú‚îÄ‚îÄ models/                    # M√¥ h√¨nh ƒë√£ l∆∞u
‚îú‚îÄ‚îÄ outputs/                   # K·∫øt qu·∫£
‚îú‚îÄ‚îÄ visualization/             # Bi·ªÉu ƒë·ªì
‚îú‚îÄ‚îÄ notebooks/                 # Jupyter notebooks
‚îî‚îÄ‚îÄ main.py                    # ·ª®ng d·ª•ng Streamlit
```

---

## üìö KI·∫æN TH·ª®C CHI TI·∫æT V·ªÄ ARIMA

### 1. ARIMA l√† g√¨?

**ARIMA** = **A**uto**R**egressive **I**ntegrated **M**oving **A**verage

L√† m√¥ h√¨nh th·ªëng k√™ d√πng ƒë·ªÉ ph√¢n t√≠ch v√† d·ª± b√°o **chu·ªói th·ªùi gian** (time series).

---

### 2. Ba th√†nh ph·∫ßn c·ªßa ARIMA

| Th√†nh ph·∫ßn | K√Ω hi·ªáu | √ù nghƒ©a |
|------------|---------|---------|
| **AR** (AutoRegressive) | **p** | H·ªìi quy d·ª±a tr√™n c√°c gi√° tr·ªã **qu√° kh·ª©** c·ªßa ch√≠nh n√≥ |
| **I** (Integrated) | **d** | S·ªë l·∫ßn **sai ph√¢n** (differencing) ƒë·ªÉ chu·ªói d·ª´ng |
| **MA** (Moving Average) | **q** | H·ªìi quy d·ª±a tr√™n **sai s·ªë d·ª± b√°o** qu√° kh·ª© |

**K√Ω hi·ªáu**: ARIMA(p, d, q)

---

### 3. C√¥ng th·ª©c to√°n h·ªçc

#### 3.1 AR (AutoRegressive) - T·ª± h·ªìi quy

$$y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + ... + \phi_p y_{t-p} + \epsilon_t$$

**Gi·∫£i th√≠ch**:
- $y_t$: Gi√° tr·ªã t·∫°i th·ªùi ƒëi·ªÉm t
- $\phi_i$: H·ªá s·ªë AR
- $\epsilon_t$: Nhi·ªÖu tr·∫Øng (white noise)
- **p**: S·ªë lag (b∆∞·ªõc l√πi) s·ª≠ d·ª•ng

**V√≠ d·ª• AR(1)**:
```
y_t = 0.8 * y_{t-1} + Œµ_t
```
‚Üí Gi√° tr·ªã h√¥m nay = 80% gi√° tr·ªã h√¥m qua + nhi·ªÖu

---

#### 3.2 I (Integrated) - Sai ph√¢n

$$y'_t = y_t - y_{t-1}$$

**M·ª•c ƒë√≠ch**: Bi·∫øn chu·ªói **kh√¥ng d·ª´ng** ‚Üí **d·ª´ng** (stationary)

| d | C√¥ng th·ª©c | T√™n g·ªçi |
|---|-----------|---------|
| d=0 | $y_t$ | Kh√¥ng sai ph√¢n |
| d=1 | $y_t - y_{t-1}$ | First difference |
| d=2 | $(y_t - y_{t-1}) - (y_{t-1} - y_{t-2})$ | Second difference |

---

#### 3.3 MA (Moving Average) - Trung b√¨nh tr∆∞·ª£t

$$y_t = c + \epsilon_t + \theta_1 \epsilon_{t-1} + \theta_2 \epsilon_{t-2} + ... + \theta_q \epsilon_{t-q}$$

**Gi·∫£i th√≠ch**:
- $\theta_i$: H·ªá s·ªë MA
- $\epsilon_{t-i}$: Sai s·ªë d·ª± b√°o t·∫°i th·ªùi ƒëi·ªÉm t-i
- **q**: S·ªë lag c·ªßa sai s·ªë s·ª≠ d·ª•ng

**V√≠ d·ª• MA(1)**:
```
y_t = Œµ_t + 0.5 * Œµ_{t-1}
```
‚Üí Gi√° tr·ªã = nhi·ªÖu hi·ªán t·∫°i + 50% nhi·ªÖu h√¥m qua

---

#### 3.4 C√¥ng th·ª©c ARIMA ƒë·∫ßy ƒë·ªß

$$y'_t = c + \phi_1 y'_{t-1} + ... + \phi_p y'_{t-p} + \theta_1 \epsilon_{t-1} + ... + \theta_q \epsilon_{t-q} + \epsilon_t$$

Trong ƒë√≥ $y'_t$ l√† chu·ªói sau khi sai ph√¢n d l·∫ßn.

---

### 4. Quy tr√¨nh x√¢y d·ª±ng ARIMA

```mermaid
flowchart TD
    A[üìä D·ªØ li·ªáu th√¥] --> B{Ki·ªÉm tra<br/>Stationarity?}
    B -->|Kh√¥ng d·ª´ng| C[üîÑ Differencing<br/>d = 1, 2, ...]
    B -->|D·ª´ng| D[üìà Ph√¢n t√≠ch ACF/PACF]
    C --> B
    D --> E[üéØ Ch·ªçn p, q]
    E --> F[üîß Fit Model]
    F --> G{Residuals<br/>ƒë·ªôc l·∫≠p?}
    G -->|Kh√¥ng| E
    G -->|C√≥| H[‚úÖ Model OK]
    H --> I[üìâ D·ª± b√°o]
```

---

### 5. Ki·ªÉm tra Stationarity (T√≠nh d·ª´ng)

#### 5.1 ƒê·∫∑c ƒëi·ªÉm chu·ªói d·ª´ng

| T√≠nh ch·∫•t | M√¥ t·∫£ |
|-----------|-------|
| **Mean kh√¥ng ƒë·ªïi** | $E[y_t] = \mu$ (h·∫±ng s·ªë) |
| **Variance kh√¥ng ƒë·ªïi** | $Var(y_t) = \sigma^2$ (h·∫±ng s·ªë) |
| **Covariance ch·ªâ ph·ª• thu·ªôc lag** | $Cov(y_t, y_{t-k})$ ch·ªâ ph·ª• thu·ªôc k |

#### 5.2 ADF Test (Augmented Dickey-Fuller)

```python
from statsmodels.tsa.stattools import adfuller

result = adfuller(series)
p_value = result[1]

if p_value < 0.05:
    print("‚úÖ Chu·ªói d·ª´ng")
else:
    print("‚ùå Chu·ªói kh√¥ng d·ª´ng ‚Üí C·∫ßn differencing")
```

**K·∫øt qu·∫£ trong project**:
- ADF tr∆∞·ªõc x·ª≠ l√Ω: p = 0.0018 ‚Üí Kh√¥ng d·ª´ng
- ADF sau log + diff: p ‚âà 0 ‚Üí D·ª´ng ‚úÖ

---

### 6. ACF v√† PACF

#### 6.1 ACF (AutoCorrelation Function)

| M√¥ h√¨nh | ACF ƒë·∫∑c tr∆∞ng |
|---------|---------------|
| AR(p) | Gi·∫£m d·∫ßn (decay) |
| MA(q) | Cut-off sau lag q |
| ARMA | Gi·∫£m d·∫ßn |

#### 6.2 PACF (Partial AutoCorrelation Function)

| M√¥ h√¨nh | PACF ƒë·∫∑c tr∆∞ng |
|---------|----------------|
| AR(p) | Cut-off sau lag p |
| MA(q) | Gi·∫£m d·∫ßn (decay) |
| ARMA | Gi·∫£m d·∫ßn |

**C√°ch ƒë·ªçc**:
```
N·∫øu PACF cut-off ·ªü lag p ‚Üí AR(p)
N·∫øu ACF cut-off ·ªü lag q ‚Üí MA(q)
```

---

### 7. Ti√™u ch√≠ ch·ªçn m√¥ h√¨nh

| Ti√™u ch√≠ | C√¥ng th·ª©c | √ù nghƒ©a |
|----------|-----------|---------|
| **AIC** | $-2\log(L) + 2k$ | C√†ng nh·ªè c√†ng t·ªët |
| **BIC** | $-2\log(L) + k\log(n)$ | Ph·∫°t n·∫∑ng h∆°n v·ªõi k l·ªõn |

Trong ƒë√≥:
- L: Likelihood
- k: S·ªë tham s·ªë
- n: S·ªë quan s√°t

**Trong project**: ARIMA(0,0,2) c√≥ AIC = -2595 (th·∫•p nh·∫•t)

---

### 8. auto_arima trong Python

```python
from pmdarima import auto_arima

model = auto_arima(
    series,
    start_p=0, start_q=0,    # B·∫Øt ƒë·∫ßu t·ª´ p=0, q=0
    max_p=3, max_q=3,         # T·ªëi ƒëa p=3, q=3
    d=None,                   # T·ª± ƒë·ªông t√¨m d
    seasonal=False,           # Kh√¥ng c√≥ seasonality
    trace=True,               # Hi·ªÉn th·ªã qu√° tr√¨nh
    suppress_warnings=True
)

# Xem k·∫øt qu·∫£
print(model.summary())
# D·ª± b√°o
forecast = model.predict(n_periods=7)
```

---

### 9. ∆Øu v√† nh∆∞·ª£c ƒëi·ªÉm ARIMA

| ‚úÖ ∆Øu ƒëi·ªÉm | ‚ùå Nh∆∞·ª£c ƒëi·ªÉm |
|-----------|--------------|
| Chu·∫©n th·ªëng k√™, d·ªÖ gi·∫£i th√≠ch | Ch·ªâ h·ªçc ƒë∆∞·ª£c quan h·ªá **tuy·∫øn t√≠nh** |
| T·ªët cho **ng·∫Øn h·∫°n** | D·ª± b√°o d√†i ‚Üí h·ªôi t·ª• v·ªÅ mean |
| √çt tham s·ªë | Kh√¥ng b·∫Øt ƒë∆∞·ª£c **regime change** |
| Baseline so s√°nh | Kh√¥ng h·ªçc ƒë∆∞·ª£c **nonlinear patterns** |

---

### 10. T·∫°i sao ARIMA cho ƒë∆∞·ªùng ph·∫≥ng?

**Hi·ªán t∆∞·ª£ng**: D·ª± ƒëo√°n 14-28 ng√†y ‚Üí t·∫•t c·∫£ gi√° tr·ªã gi·ªëng nhau

**Nguy√™n nh√¢n to√°n h·ªçc**:

```
D·ª± ƒëo√°n ARIMA = T·ªï h·ª£p tuy·∫øn t√≠nh c·ªßa qu√° kh·ª© + E[Œµ] = 0

Khi d·ª± ƒëo√°n xa:
- Kh√¥ng c√≥ th√¥ng tin m·ªõi
- M√¥ h√¨nh h·ªôi t·ª• v·ªÅ E[y] (mean)
- ‚Üí ƒê∆∞·ªùng ph·∫≥ng
```

```mermaid
graph LR
    A[D·ª± b√°o ng·∫Øn h·∫°n<br/>1-7 ng√†y] -->|‚úÖ OK| B[C√≥ variation]
    C[D·ª± b√°o d√†i h·∫°n<br/>14-28 ng√†y] -->|‚ùå| D[Mean reversion<br/>ƒê∆∞·ªùng ph·∫≥ng]
```

---

### 11. So s√°nh v·ªõi SARIMA

| ƒê·∫∑c ƒëi·ªÉm | ARIMA | SARIMA |
|----------|-------|--------|
| **Seasonal** | Kh√¥ng | C√≥ |
| **K√Ω hi·ªáu** | (p,d,q) | (p,d,q)(P,D,Q)m |
| **Ph√π h·ª£p** | D·ªØ li·ªáu kh√¥ng m√πa | D·ªØ li·ªáu c√≥ m√πa (monthly, yearly) |

**SARIMA**: ARIMA + Seasonal component
- m = seasonal period (12 cho monthly, 4 cho quarterly)

---

### 12. Code trong Project

```python
# File: scripts/models/arima/arima.py

from pmdarima import auto_arima
from sklearn.metrics import mean_absolute_error

# Load data
series = pd.read_csv("data/processed/model_ready/usd_series.csv")["usd_index"]

# Auto-fit ARIMA
model = auto_arima(
    series,
    start_p=0, start_q=0,
    max_p=3, max_q=3,
    d=0,                    # ƒê√£ differencing tr∆∞·ªõc ƒë√≥
    seasonal=False,
    suppress_warnings=True
)

# Predict
forecast = model.predict(n_periods=7)

# Evaluate
mae = mean_absolute_error(y_true, forecast)
print(f"MAE: {mae:.4f}")
```

---

### 13. Inverse Transform khi d·ª± ƒëo√°n

V√¨ ƒë√£ √°p d·ª•ng **log + differencing**, c·∫ßn inverse:

```python
# D·ª± b√°o tr√™n chu·ªói ƒë√£ diff
pred_diff = model.predict(n_periods=7)

# Inverse differencing
last_log = np.log(last_value)
pred_log = last_log + np.cumsum(pred_diff)

# Inverse log
pred_actual = np.exp(pred_log)
```

---

## üß† KI·∫æN TH·ª®C CHI TI·∫æT V·ªÄ LSTM

### 1. LSTM l√† g√¨?

**LSTM** = **L**ong **S**hort-**T**erm **M**emory

L√† m·ªôt lo·∫°i **Recurrent Neural Network (RNN)** ƒë·∫∑c bi·ªát, ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ h·ªçc **ph·ª• thu·ªôc d√†i h·∫°n** trong chu·ªói th·ªùi gian.

**V·∫•n ƒë·ªÅ c·ªßa RNN th√¥ng th∆∞·ªùng**:
- **Vanishing gradient**: Gradient nh·ªè d·∫ßn khi backprop qua nhi·ªÅu b∆∞·ªõc
- **Kh√¥ng nh·ªõ ƒë∆∞·ª£c th√¥ng tin xa**

**LSTM gi·∫£i quy·∫øt**: S·ª≠ d·ª•ng c∆° ch·∫ø **gate** ƒë·ªÉ ki·ªÉm so√°t d√≤ng th√¥ng tin.

---

### 2. Ki·∫øn tr√∫c LSTM Cell

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ             LSTM Cell                  ‚îÇ
    c_{t-1} ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> c_t
                    ‚îÇ  ‚îÇ  √ó  ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÇ  f  ‚îÇ Forget Gate        ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îÇ
                    ‚îÇ     ‚îÇ       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
                    ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ  +  ‚îÇ<‚îÄ‚îÄ‚îÄ‚îÇ  √ó  ‚îÇ        ‚îÇ
                    ‚îÇ             ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò        ‚îÇ
                    ‚îÇ                ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
                    ‚îÇ             ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê ‚îÇ                  ‚îÇ
    h_{t-1} ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ             ‚îÇtanh ‚îÇ ‚îÇ                  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ> h_t
                    ‚îÇ             ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îÇ                  ‚îÇ
     x_t ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>‚îÇ                ‚îÇ    ‚îÇ                  ‚îÇ
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îê ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îÇ
                    ‚îÇ  ‚îÇ  i  ‚îÇ‚îÄ‚îÄ‚îÄ>‚îÇ  √ó  ‚îÇ<‚îò ‚îÇ  o  ‚îÇ         ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ
                    ‚îÇ Input Gate           Output Gate       ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 3. Ba lo·∫°i Gate trong LSTM

| Gate | K√Ω hi·ªáu | Ch·ª©c nƒÉng | C√¥ng th·ª©c |
|------|---------|-----------|-----------|
| **Forget Gate** | $f_t$ | Quy·∫øt ƒë·ªãnh **b·ªè** th√¥ng tin n√†o | $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$ |
| **Input Gate** | $i_t$ | Quy·∫øt ƒë·ªãnh **th√™m** th√¥ng tin n√†o | $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$ |
| **Output Gate** | $o_t$ | Quy·∫øt ƒë·ªãnh **xu·∫•t** th√¥ng tin n√†o | $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$ |

**Trong ƒë√≥**: $\sigma$ = sigmoid function (gi√° tr·ªã 0-1)

---

### 4. C√¥ng th·ª©c to√°n h·ªçc LSTM

#### B∆∞·ªõc 1: Forget Gate - Qu√™n th√¥ng tin c≈©
$$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$$

#### B∆∞·ªõc 2: Input Gate - Ch·ªçn th√¥ng tin m·ªõi
$$i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)$$
$$\tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C)$$

#### B∆∞·ªõc 3: C·∫≠p nh·∫≠t Cell State
$$C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$$

#### B∆∞·ªõc 4: Output Gate - Xu·∫•t output
$$o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)$$
$$h_t = o_t \odot \tanh(C_t)$$

**K√Ω hi·ªáu**:
- $\odot$: Element-wise multiplication
- $C_t$: Cell state (b·ªô nh·ªõ d√†i h·∫°n)
- $h_t$: Hidden state (output)

---

### 5. LSTM cho Time Series

```mermaid
flowchart LR
    A[Input Sequence<br/>30 days] --> B[LSTM Layer<br/>32 units]
    B --> C[Dense Layer<br/>1 unit]
    C --> D[Output<br/>Prediction]
```

**Input shape**: [(batch_size, time_steps, features)](file:///d:/FPT/k%C3%AC%207/DAT/exchange-rate/scripts/preprocessing/feature_engineering.py#7-13) = [(N, 30, 1)](file:///d:/FPT/k%C3%AC%207/DAT/exchange-rate/scripts/preprocessing/feature_engineering.py#7-13)

**Output**: Gi√° tr·ªã d·ª± ƒëo√°n ng√†y th·ª© 38

---

### 6. Code LSTM trong Project

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Load windowed data
X = np.load("data/processed/model_ready/X.npy")  # (N, 30)
y = np.load("data/processed/model_ready/y.npy")  # (N,)

# Reshape for LSTM: (N, 30, 1)
X = X[..., None]

# Build model
model = Sequential([
    LSTM(32, input_shape=(30, 1)),  # 32 hidden units
    Dense(1)                         # Output layer
])

model.compile(optimizer="adam", loss="mse")
model.fit(X_train, y_train, epochs=10, validation_split=0.2)

# Predict
predictions = model.predict(X_test)
```

---

### 7. Hyperparameters quan tr·ªçng

| Hyperparameter | M√¥ t·∫£ | Gi√° tr·ªã trong project |
|----------------|-------|----------------------|
| **Units** | S·ªë neurons trong LSTM layer | 32 |
| **Layers** | S·ªë LSTM layers | 1 |
| **Dropout** | Regularization | 0 (c√≥ th·ªÉ th√™m) |
| **Optimizer** | Thu·∫≠t to√°n t·ªëi ∆∞u | Adam |
| **Loss** | H√†m m·∫•t m√°t | MSE |
| **Epochs** | S·ªë l·∫ßn train | 10 |

---

### 8. T·∫°i sao LSTM t·ªët h∆°n ARIMA?

| Ti√™u ch√≠ | ARIMA | LSTM |
|----------|-------|------|
| **Quan h·ªá** | Tuy·∫øn t√≠nh | **Phi tuy·∫øn** ‚úÖ |
| **Memory** | Gi·ªõi h·∫°n (p lags) | **Long-term** ‚úÖ |
| **Pattern ph·ª©c t·∫°p** | ‚ùå Kh√¥ng | ‚úÖ H·ªçc ƒë∆∞·ª£c |
| **Regime change** | ‚ùå Kh√¥ng | ‚úÖ C√≥ th·ªÉ |
| **Gi·∫£i th√≠ch** | ‚úÖ D·ªÖ | ‚ùå Black box |

---

### 9. Stacked LSTM (N√¢ng cao)

```python
model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(30, 1)),
    LSTM(32, return_sequences=False),
    Dense(16, activation='relu'),
    Dense(1)
])
```

**Khi n√†o d√πng**: D·ªØ li·ªáu c√≥ nhi·ªÅu pattern ph·ª©c t·∫°p, c·∫ßn nhi·ªÅu abstraction layers.

---

### 10. Bidirectional LSTM

```python
from tensorflow.keras.layers import Bidirectional

model = Sequential([
    Bidirectional(LSTM(32), input_shape=(30, 1)),
    Dense(1)
])
```

**√ù nghƒ©a**: H·ªçc pattern t·ª´ c·∫£ **qu√° kh·ª© ‚Üí t∆∞∆°ng lai** v√† **t∆∞∆°ng lai ‚Üí qu√° kh·ª©**.

---

## ü§ñ KI·∫æN TH·ª®C CHI TI·∫æT V·ªÄ TRANSFORMER

### 1. Transformer l√† g√¨?

**Transformer** l√† ki·∫øn tr√∫c deep learning ƒë∆∞·ª£c gi·ªõi thi·ªáu trong paper "Attention Is All You Need" (2017).

**ƒê·∫∑c ƒëi·ªÉm ch√≠nh**:
- **Kh√¥ng d√πng RNN/LSTM** (kh√¥ng sequential)
- D·ª±a tr√™n c∆° ch·∫ø **Self-Attention**
- **Song song h√≥a** t·ªët ‚Üí train nhanh

---

### 2. Self-Attention Mechanism

**√ù t∆∞·ªüng**: M·ªói v·ªã tr√≠ trong chu·ªói "nh√¨n" v√†o t·∫•t c·∫£ c√°c v·ªã tr√≠ kh√°c ƒë·ªÉ hi·ªÉu context.

```
"T·ª∑ gi√° EUR h√¥m nay ph·ª• thu·ªôc v√†o JPY tu·∫ßn tr∆∞·ªõc"
     ‚îÇ                              ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Attention ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### 3. C√¥ng th·ª©c Attention

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

**Trong ƒë√≥**:
- **Q** (Query): "T√¥i ƒëang t√¨m g√¨?"
- **K** (Key): "T√¥i c√≥ th√¥ng tin g√¨?"
- **V** (Value): "Gi√° tr·ªã th·ª±c s·ª±"
- **$d_k$**: Dimension c·ªßa key (ƒë·ªÉ scale)

---

### 4. Multi-Head Attention

Thay v√¨ 1 attention, d√πng **nhi·ªÅu heads** song song:

$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O$$

$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

**L·ª£i √≠ch**: H·ªçc ƒë∆∞·ª£c nhi·ªÅu lo·∫°i quan h·ªá kh√°c nhau (short-term, long-term, trend, volatility...)

---

### 5. Ki·∫øn tr√∫c Transformer cho Time Series

```mermaid
flowchart TD
    A[Input: 30 days √ó 7 currencies] --> B[Positional Encoding]
    B --> C[Multi-Head Attention]
    C --> D[Add & Norm]
    D --> E[Feed Forward]
    E --> F[Add & Norm]
    F --> G[Output Layer]
    G --> H[Prediction: 7 days ahead]
```

---

### 6. Positional Encoding

V√¨ Transformer kh√¥ng c√≥ kh√°i ni·ªám "th·ª© t·ª±", c·∫ßn th√™m **positional encoding**:

$$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)$$
$$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$

**√ù nghƒ©a**: M√£ h√≥a v·ªã tr√≠ c·ªßa m·ªói time step trong chu·ªói.

---

### 7. So s√°nh LSTM vs Transformer

| Ti√™u ch√≠ | LSTM | Transformer |
|----------|------|-------------|
| **C√°ch x·ª≠ l√Ω sequence** | Tu·∫ßn t·ª± | **Song song** ‚úÖ |
| **Long-range dependency** | Kh√≥ | **D·ªÖ** (attention) ‚úÖ |
| **Multivariate** | C√≥ th·ªÉ | **T·ª± nhi√™n** ‚úÖ |
| **T·ªëc ƒë·ªô train** | Ch·∫≠m | **Nhanh** ‚úÖ |
| **Data c·∫ßn** | √çt | **Nhi·ªÅu** |
| **Interpretability** | ‚ùå | **Attention weights** ‚úÖ |

---

### 8. Code Transformer c∆° b·∫£n

```python
import torch
import torch.nn as nn

class TimeSeriesTransformer(nn.Module):
    def __init__(self, input_dim=7, d_model=64, nhead=4, 
                 num_layers=2, dim_feedforward=256):
        super().__init__()
        
        # Input embedding
        self.embedding = nn.Linear(input_dim, d_model)
        
        # Positional encoding
        self.pos_encoder = PositionalEncoding(d_model)
        
        # Transformer encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=dim_feedforward
        )
        self.transformer = nn.TransformerEncoder(
            encoder_layer, 
            num_layers=num_layers
        )
        
        # Output layer
        self.fc = nn.Linear(d_model, 1)
    
    def forward(self, x):
        # x: (batch, seq_len, features)
        x = self.embedding(x)
        x = self.pos_encoder(x)
        x = self.transformer(x)
        x = self.fc(x[:, -1, :])  # Ch·ªâ l·∫•y output cu·ªëi
        return x
```

---

### 9. Input cho Transformer (Multivariate)

```python
# 7 currencies √ó 30 days
X_shape = (batch_size, 30, 7)

# C√°c features:
features = [
    'euro_to_usd',
    'japanese_yen_to_usd',
    'uk_pound_to_usd',
    'swiss_franc_to_usd',
    'australian_dollar_to_usd',
    'canadian_dollar_to_usd',
    'chinese_yuan_to_usd'
]
```

**L·ª£i th·∫ø**: Transformer t·ª± ƒë·ªông h·ªçc **quan h·ªá gi·ªØa c√°c currencies** qua Attention.

---

### 10. Attention Visualization

Transformer cho ph√©p **visualize attention weights** ƒë·ªÉ hi·ªÉu m√¥ h√¨nh:

```python
# L·∫•y attention weights
attention_weights = model.get_attention_weights(x)

# Visualize
plt.imshow(attention_weights, cmap='viridis')
plt.xlabel("Key positions (time steps)")
plt.ylabel("Query positions")
plt.title("Attention Heatmap")
```

**√ù nghƒ©a**:
- Xem model ƒëang "ch√∫ √Ω" v√†o time steps n√†o
- Gi·∫£i th√≠ch t·∫°i sao model ƒë∆∞a ra d·ª± ƒëo√°n

---

### 11. C√°c bi·∫øn th·ªÉ Transformer cho Time Series

| Model | ƒê·∫∑c ƒëi·ªÉm |
|-------|----------|
| **Informer** | Sparse attention cho sequence d√†i |
| **Autoformer** | Series decomposition |
| **FEDformer** | Frequency domain learning |
| **PatchTST** | Patch-based cho multivariate |
| **Temporal Fusion Transformer** | Attention + variable selection |

---

### 12. T·∫°i sao Transformer cho d·ª± ƒëo√°n t·ª∑ gi√°?

| L·ª£i th·∫ø | Gi·∫£i th√≠ch |
|---------|------------|
| **Multi-currency learning** | H·ªçc m·ªëi quan h·ªá EUR-JPY-GBP... ƒë·ªìng th·ªùi |
| **Shock detection** | Attention c√≥ th·ªÉ nh√¨n xa ƒë·ªÉ detect shock |
| **Lead-lag effects** | M·ªôt currency c√≥ th·ªÉ "d·∫´n d·∫Øt" currency kh√°c |
| **Regime change** | Attention weights thay ƒë·ªïi theo regime |

---

### 13. Pipeline ho√†n ch·ªânh

```mermaid
flowchart TD
    A[Raw Data<br/>7 currencies] --> B[Build USD Index<br/>+ Preprocessing]
    B --> C{Ch·ªçn Model}
    C --> D[ARIMA<br/>Baseline]
    C --> E[LSTM<br/>Univariate]
    C --> F[Transformer<br/>Multivariate]
    D --> G[Compare Results]
    E --> G
    F --> G
    G --> H[Best Model<br/>for Deployment]
```

---

## üìã C√ÅC SLIDE ƒê·ªÄ XU·∫§T

### **SLIDE 1: Trang b√¨a**
- T√™n ƒë·ªÅ t√†i: "D·ª± ƒëo√°n s·ª©c m·∫°nh ƒë·ªìng USD s·ª≠ d·ª•ng Machine Learning"
- Th√†nh vi√™n nh√≥m
- M√¥n h·ªçc: DAT
- Th·ªùi gian

---

### **SLIDE 2: Gi·ªõi thi·ªáu b√†i to√°n**
- **V·∫•n ƒë·ªÅ**: D·ª± ƒëo√°n xu h∆∞·ªõng t·ª∑ gi√° ngo·∫°i h·ªëi
- **M·ª•c ti√™u**: D·ª± ƒëo√°n USD Index trong 7 ng√†y t·ªõi
- **√ù nghƒ©a th·ª±c ti·ªÖn**: Trading, qu·∫£n l√Ω r·ªßi ro t√†i ch√≠nh

---

### **SLIDE 3: D·ªØ li·ªáu s·ª≠ d·ª•ng**

| ƒê·∫∑c ƒëi·ªÉm | Gi√° tr·ªã |
|----------|---------|
| **Ngu·ªìn d·ªØ li·ªáu** | 7 lo·∫°i t·ª∑ gi√° major currencies |
| **Th·ªùi gian** | 2004 - 2026 |
| **C√°c ƒë·ªìng ti·ªÅn** | EUR, JPY, GBP, CHF, AUD, CAD, CNY |

**File**: [exchange_rate_to_usd.csv](file:///d:/FPT/k√¨%207/DAT/exchange-rate/data/raw/exchange_rate_to_usd.csv)

---

### **SLIDE 4: X√¢y d·ª±ng USD Index**

```
USD_index(t) = mean(EUR, JPY, GBP, CHF, AUD, CAD, CNY)
```

**L√Ω do**:
- Ki·ªÉm so√°t ngu·ªìn d·ªØ li·ªáu
- Hi·ªÉu r√µ c·∫•u tr√∫c feature
- Ph√π h·ª£p cho m·ªü r·ªông multivariate

**Script**: [build_usd_index.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/preprocessing/build_usd_index.py)

---

### **SLIDE 5: Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu**

```mermaid
flowchart LR
    A[Raw Data] --> B[Clean & Merge]
    B --> C[Build USD Index]
    C --> D[Log Transform]
    D --> E[Differencing]
    E --> F[Stationarity Check]
```

**C√°c b∆∞·ªõc**:
1. L√†m s·∫°ch d·ªØ li·ªáu ‚Üí [clean_data.py](file:///d:/FPT/k%C3%AC%207/DAT/exchange-rate/scripts/preprocessing/clean_data.py)
2. T·∫°o USD Index ‚Üí [build_usd_index.py](file:///d:/FPT/k%C3%AC%207/DAT/exchange-rate/scripts/preprocessing/build_usd_index.py)
3. Ki·ªÉm tra t√≠nh d·ª´ng (ADF test) ‚Üí [stationarity.py](file:///d:/FPT/k%C3%AC%207/DAT/exchange-rate/scripts/preprocessing/stationarity.py)
4. Feature Engineering ‚Üí [feature_engineering.py](file:///d:/FPT/k%C3%AC%207/DAT/exchange-rate/scripts/preprocessing/feature_engineering.py)

---

### **SLIDE 6: Ki·ªÉm tra Stationarity**

| Ki·ªÉm tra | K·∫øt qu·∫£ |
|----------|---------|
| ADF tr∆∞·ªõc x·ª≠ l√Ω | p-value = 0.0018 ‚Üí Kh√¥ng d·ª´ng |
| ADF sau log + diff | p-value ‚âà 0 ‚Üí ƒê√£ d·ª´ng ‚úÖ |

**Ph∆∞∆°ng ph√°p**:
- Log transform
- First differencing

---

### **SLIDE 7: M√¥ h√¨nh ARIMA (Baseline)**

**Th√¥ng tin m√¥ h√¨nh**:
| Tham s·ªë | Gi√° tr·ªã |
|---------|---------|
| Model | ARIMA(0,0,2) |
| AIC | -2595 |
| MAE | ‚âà 0.052 |

**File**: [arima.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/models/arima/arima.py)

---

### **SLIDE 8: ƒê√°nh gi√° ARIMA**

| Ph∆∞∆°ng ph√°p | M√¥ t·∫£ |
|-------------|-------|
| **Hold-out** | 80% train / 20% test |
| **Rolling Backtest** | Train l·∫°i m·ªói b∆∞·ªõc, d·ª± ƒëo√°n 7 ng√†y |

**File**: [backtest_arima.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/evaluation/backtest_arima.py)

---

### **SLIDE 9: H·∫°n ch·∫ø c·ªßa ARIMA**

> [!WARNING]
> ARIMA d·ª± ƒëo√°n d√†i h·∫°n ‚Üí ƒê∆∞·ªùng ph·∫≥ng (Mean Reversion)

**Nguy√™n nh√¢n**:
- M√¥ h√¨nh tuy·∫øn t√≠nh
- Kh√¥ng h·ªçc ƒë∆∞·ª£c pattern phi tuy·∫øn
- H·ªôi t·ª• v·ªÅ gi√° tr·ªã trung b√¨nh

**K·∫øt lu·∫≠n**: C·∫ßn m√¥ h√¨nh h·ªçc s√¢u (Deep Learning)

---

### **SLIDE 10: M√¥ h√¨nh LSTM**

**Ki·∫øn tr√∫c**:
```
Input(30 days) ‚Üí LSTM(32 units) ‚Üí Dense(1) ‚Üí Output
```

**∆Øu ƒëi·ªÉm so v·ªõi ARIMA**:
| ARIMA | LSTM |
|-------|------|
| Linear | Nonlinear |
| Kh√¥ng nh·ªõ d√†i h·∫°n | Long-term memory |
| H·ªôi t·ª• mean | H·ªçc pattern ph·ª©c t·∫°p |

**File**: [lstm.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/models/lstm.py)

---

### **SLIDE 11: Feature Engineering cho Deep Learning**

```python
# Window: 30 ng√†y ‚Üí D·ª± ƒëo√°n: ng√†y th·ª© 38
X: (N, 30, 1)  # N samples, 30 time steps, 1 feature
y: (N,)        # Target value
```

**File**: [feature_engineering.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/preprocessing/feature_engineering.py)

---

### **SLIDE 12: M√¥ h√¨nh Transformer (ƒê·ªÅ xu·∫•t)**

**∆Øu ƒëi·ªÉm**:
- Attention mechanism to√†n chu·ªói
- Kh√¥ng b·ªã gi·ªõi h·∫°n c·ª≠a s·ªï
- H·ªçc ƒë∆∞·ª£c:
  - Quan h·ªá gi·ªØa c√°c currency
  - Shock to√†n c·ª•c
  - Lead-lag effects

**Input**: (N, 30, 7) - Multivariate v·ªõi 7 currencies

---

### **SLIDE 13: So s√°nh c√°c m√¥ h√¨nh**

```mermaid
graph TD
    A[ARIMA<br/>Baseline Linear] --> B{K·∫øt qu·∫£?}
    B -->|Ng·∫Øn h·∫°n OK| C[‚úÖ Baseline t·ªët]
    B -->|D√†i h·∫°n Flat| D[‚ùå C·∫ßn DL]
    D --> E[LSTM<br/>Nonlinear + Memory]
    E --> F[Transformer<br/>Attention + Multivariate]
```

| M√¥ h√¨nh | ƒêi·ªÉm m·∫°nh | ƒêi·ªÉm y·∫øu |
|---------|-----------|----------|
| ARIMA | D·ªÖ hi·ªÉu, baseline | Linear, short-term only |
| LSTM | Nonlinear, memory | Single variable |
| Transformer | Attention, multi-var | Ph·ª©c t·∫°p |

---

### **SLIDE 14: Demo ·ª©ng d·ª•ng**

**Streamlit Application**:
- D·ª± ƒëo√°n USD Index v·ªõi slider ch·ªçn horizon
- Hi·ªÉn th·ªã chart v√† b·∫£ng d·ªØ li·ªáu
- Interpretation t·ª± ƒë·ªông

**File**: [main.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/main.py)

**Ch·∫°y**: `streamlit run main.py`

---

### **SLIDE 15: K·∫øt lu·∫≠n**

> [!IMPORTANT]
> **K·∫øt qu·∫£ ch√≠nh**:
> 1. ‚úÖ ARIMA fit t·ªët cho short-term
> 2. ‚ùå ARIMA kh√¥ng h·ªçc ƒë∆∞·ª£c nonlinear patterns
> 3. ‚úÖ LSTM/Transformer c·∫ßn thi·∫øt cho long-term prediction

**H∆∞·ªõng ph√°t tri·ªÉn**:
- Ho√†n thi·ªán Transformer multivariate
- Th√™m attention visualization
- Deploy production

---

### **SLIDE 16: Q&A**
- C√¢u h·ªèi & Th·∫£o lu·∫≠n

---

## üìÅ FILE QUAN TR·ªåNG C·∫¶N THAM KH·∫¢O

### Preprocessing
| File | M√¥ t·∫£ |
|------|-------|
| [build_usd_index.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/preprocessing/build_usd_index.py) | X√¢y d·ª±ng USD Index t·ª´ 7 currencies |
| [feature_engineering.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/preprocessing/feature_engineering.py) | T·∫°o windowed data cho LSTM |
| [stationarity.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/preprocessing/stationarity.py) | Ki·ªÉm tra ADF test |

### Models
| File | M√¥ t·∫£ |
|------|-------|
| [arima.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/models/arima/arima.py) | ARIMA backtest |
| [lstm.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/models/lstm.py) | LSTM model |
| [transformer.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/models/transformer.py) | Transformer (ƒëang ph√°t tri·ªÉn) |

### Evaluation
| File | M√¥ t·∫£ |
|------|-------|
| [backtest_arima.py](file:///d:/FPT/k√¨%207/DAT/exchange-rate/scripts/evaluation/backtest_arima.py) | Rolling backtest cho ARIMA |

### Documentation
| File | M√¥ t·∫£ |
|------|-------|
| [readme_arima.md](file:///d:/FPT/k√¨%207/DAT/exchange-rate/readme_arima.md) | T√†i li·ªáu chi ti·∫øt v·ªÅ ARIMA |

---

## üìä BI·ªÇU ƒê·ªí C√ì S·∫¥N

C√°c h√¨nh ·∫£nh c√≥ th·ªÉ d√πng cho slide:
- [arima_forecast.png](file:///d:/FPT/k%C3%AC%207/DAT/exchange-rate/arima_forecast.png) - Bi·ªÉu ƒë·ªì d·ª± b√°o ARIMA
- [backtest.png](file:///d:/FPT/k%C3%AC%207/DAT/exchange-rate/backtest.png) - K·∫øt qu·∫£ backtest
- `rolling back arima.png` - Rolling backtest
- [image-1.png](file:///d:/FPT/k%C3%AC%207/DAT/exchange-rate/image-1.png) - H√¨nh minh h·ªça

---

## üõ†Ô∏è C√ÅCH CH·∫†Y PROJECT

```bash
# 1. C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt

# 2. Ch·∫°y preprocessing
python scripts/preprocessing/build_usd_index.py

# 3. Ch·∫°y ARIMA
python scripts/models/arima/arima.py

# 4. Ch·∫°y LSTM
python scripts/models/lstm.py

# 5. Ch·∫°y Demo App
streamlit run main.py
```

---

## üí° G·ª¢I √ù THI·∫æT K·∫æ SLIDE

1. **M√†u s·∫Øc**: S·ª≠ d·ª•ng t√¥ng xanh d∆∞∆°ng (t√†i ch√≠nh) + tr·∫Øng
2. **Font**: Roboto ho·∫∑c Inter cho hi·ªán ƒë·∫°i
3. **Charts**: S·ª≠ d·ª•ng c√°c h√¨nh c√≥ s·∫µn trong project
4. **Animation**: Reveal t·ª´ng ƒëi·ªÉm khi tr√¨nh b√†y
5. **Template**: Clean, professional style
